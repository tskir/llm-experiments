{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58be9dbc",
   "metadata": {},
   "source": [
    "<big><h1>Named Entity Recognition on EMBO SourceData-NLP</h1>\n",
    "\n",
    "<big><i>By Kirill Tsukanov, ktsukanov@ebi.ac.uk</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f278e",
   "metadata": {},
   "source": [
    "# 1. Scope of this notebook\n",
    "<big>Training and evaluating ML models can be quite resource intensive. This being a technical exercise, I wanted to keep this notebook consise, so that I can showcase the principles while avoiding getting bogged down in technical details.\n",
    "\n",
    "<big>I also wanted this notebook to be able to run in reasonable time on a modern machine, so I had to simplify some things. To highlight these, and to explain how I would have done things differently for a serious production implementation, I added notes throughout this notebook.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<big>üñõ These notes will look like this.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87399439",
   "metadata": {},
   "source": [
    "# 2. Dataset selection\n",
    "<big>I used two resources to search for datasets: [Hugging Face](https://huggingface.co/datasets) and [Papers With Code](https://paperswithcode.com/datasets). While scouting the latter for the most recent datasets tagged as ‚ÄúNER‚Äù and ‚ÄúBiomedical‚Äù, I found this really interesting one: [**EMBO SourceData-NLP**](https://paperswithcode.com/paper/the-sourcedata-nlp-dataset-integrating).\n",
    "\n",
    "## 2.1. Why SourceData-NLP?\n",
    "<big>There are several things that I liked about it:\n",
    "* <big>**Recent.** First release in March 2023, latest update in September 2023.\n",
    "* <big>**Open.** The dataset is published under the permissive [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/deed.en) license.\n",
    "* <big>**Manually curated.** All entries in the dataset are added by human curators, with spot checks from a second curator.\n",
    "* <big>**Reputable source.** The dataset is published by [EMBO](https://www.embo.org/) (European Molecular Biology Organization), closely affiliated with EMBL.\n",
    "\n",
    "<big>And most importantly...\n",
    "\n",
    "## 2.2. Novel approach\n",
    "<big>While other datasets focus on abstracts (easy to curate = higher numbers, but limited information) or full texts (lots of information, but hard to curate = lower numbers), this one focuses on figure captions. The authors give the following rationale in the paper:\n",
    "* <big>The _primary_ source of most novel biomedical findings in the papers are **figures,** like cell microphotographs or assay results.\n",
    "* <big>The _next closest_ source which we can process with NLP are **figure captions** describing those experimental results.\n",
    "* <big>Figure captions use standard language and format, are not very long, and have high information density.\n",
    "\n",
    "## 2.3. Organisational considerations\n",
    "<big>Finally, I liked the organisational approach the authors took to curating the dataset: the curation was done at the same time as accepting the papers for publication. This guarantees very up-to-date information in the dataset, and also curators can (and do) contact authors of the paper if they are unsure how to interpret anything in the figure caption.\n",
    "\n",
    "## 2.4. Dataset overview\n",
    "<big>Statistics:\n",
    "- <big>25 scientific journals\n",
    "- <big>3 223 papers\n",
    "- <big>18 689 figures\n",
    "- <big>62 543 panels (semantically distinct parts of figures)\n",
    "- <big>661 862 curated entities\n",
    "  + Out of them, 623 162 linked to ontologies\n",
    "\n",
    "<big>Types of curated entities:\n",
    "* <big>Gene products\n",
    "* <big>Chemicals or small molecules\n",
    "* <big>Cells\n",
    "* <big>Tissues\n",
    "* <big>Subcellular locations\n",
    "* <big>Organism or species\n",
    "* <big>Experimental assay\n",
    "* <big>Disease\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<big>üñõ While the dataset is novel and interesting, the authors themselves note in the paper that it has some limitations. For example, certain classes tend to be underrepresented in figure captions. This is especially true for diseases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa14752",
   "metadata": {},
   "source": [
    "# 3. Technical setup\n",
    "<big>This section sets up the necessary software, environment, and modules to run the notebook. This notebook was tested on Ubuntu 22.04 LTS. However, with minor modifications (see comments in the next two blocks) it should be possible to run it on any platform where Python is supported.\n",
    "\n",
    "<big>The machine which ran the code had a 12 core CPU, 64 GB of RAM, and a NVIDIA GeForce 3080 12GB GPU.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<big>üñõ I am aware that the use of GPUs in academia is associated with certain controversies and sustainability concerns. Since I did have a GPU at hand, I decided to use for this exercise to speed up the computations. However, in principle all commands could be run on a CPU as well, as PyTorch provides a seamless interface to switch between them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ba1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Python and core modules.\n",
    "# # NOTE: syntax is specific to Ubuntu/Debian and will differ for other platforms.\n",
    "# ! sudo apt -qqq update\n",
    "# ! sudo apt -qqq install -y \\\n",
    "#     python3 \\\n",
    "#     python3-pip \\\n",
    "#     python3-venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdae642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the virtual environment.\n",
    "# NOTE: syntax is specific to Linux/Mac and will differ for other platforms.\n",
    "! python3 -m venv env\n",
    "! source env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a50a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python libraries.\n",
    "! pip -q install --no-warn-script-location \\\n",
    "    accelerate==0.26.1 \\\n",
    "    datasets==2.16.1 \\\n",
    "    ipywidgets==8.1.1 \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    pandarallel==1.6.5 \\\n",
    "    pandas==2.2.0 \\\n",
    "    scikit-learn==1.4.0 \\\n",
    "    scipy==1.12.0 \\\n",
    "    seaborn==0.13.2 \\\n",
    "    torch==2.2.0 \\\n",
    "    transformers==4.37.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import built-in modules.\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Import third-party modules.\n",
    "import datasets\n",
    "with warnings.catch_warnings():\n",
    "    # Suppress matplotlib Axes3D warning.\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, DistilBertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable parallel Pandas computation.\n",
    "pandarallel.initialize(nb_workers=os.cpu_count())\n",
    "\n",
    "# Suppress transformer warnings (for example, that a newly instantiated model has not been trained).\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21d9a4",
   "metadata": {},
   "source": [
    "# 4. Fetch and investigate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e30998",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\n",
    "    \"EMBO/SourceData\",\n",
    "    \"NER\",\n",
    "    version=\"2.0.3\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ab23e",
   "metadata": {},
   "source": [
    "## 4.1. Basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea80f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dba944",
   "metadata": {},
   "source": [
    "<big>We can see that the dataset is already split into train/test/validation subsets. I will use them throughout this notebook. In order to avoid any subconscious bias, and in line with best practices for developing ML pipelines, I will never look directly inside the test and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f81cc",
   "metadata": {},
   "source": [
    "## 4.2. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a7f5b",
   "metadata": {},
   "source": [
    "## 4.3. Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"].features[\"labels\"].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727e8a2",
   "metadata": {},
   "source": [
    "<big>We can see that the labels are annotated in the standard IOB (inside-outside-beginning) notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b64fc",
   "metadata": {},
   "source": [
    "# 5. Preprocess data\n",
    "<big>The data is currently broken down into words. In order to be able to run ML models, we need to tokenise them. Because the number of sub-word tokens isn't going to match the number of original words (and by consequence, labels), we need to make sure the labels are updated following the transformation.\n",
    "\n",
    "## 5.1. Convert into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db644c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a Pandas DataFrame.\n",
    "df = pd.DataFrame(ds['train'])\n",
    "\n",
    "# Define all data subsets to make operating on them easier.\n",
    "ALL_SUBSETS = (\"train\", \"validation\", \"test\")\n",
    "df = dict()\n",
    "\n",
    "# Select the columns we are going to need.\n",
    "for subset in ALL_SUBSETS:\n",
    "    df[subset] = pd.DataFrame(ds[subset])[[\"words\", \"labels\"]]\n",
    "\n",
    "df[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212829d8",
   "metadata": {},
   "source": [
    "## 5.2. Recover word IOB labels\n",
    "<big>Since the tokens represent sub-words, their number is going to be greater than the original number of words (and correspondingly, labels). Because of that, we need to accurately transform the word labels into token labels. But in order to do that, we need to first recover the original IOB word labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ds[\"train\"].features[\"labels\"].feature.names\n",
    "\n",
    "def recover_labels(labels):\n",
    "    return [LABELS[x] for x in labels]\n",
    "\n",
    "for subset in ALL_SUBSETS:\n",
    "    df[subset][\"labels_str\"] = df[subset][\"labels\"].parallel_apply(recover_labels)\n",
    "\n",
    "df[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc6001",
   "metadata": {},
   "source": [
    "## 5.3. Tokenise and compute token IOB labels\n",
    "<big>At this point, we can apply the tokeniser. It's essential to adjust the labels, as the number of tokens is going to be greater than the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461635c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained tokeniser.\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokeniser = BertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34983d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_word_label(label, repetitions):\n",
    "    \"\"\"Convert word IOB labels into token IOB labels for a given word.\"\"\"\n",
    "\n",
    "    if label == \"O\" or label.startswith(\"I-\"):\n",
    "        # Inside and outside labels are just repeated with no modifications.\n",
    "        return [label] * repetitions\n",
    "\n",
    "    if label.startswith(\"B-\"):\n",
    "        # For a \"before\" label, we need to convert into `1` of \"before\" and `repetitions-1` of \"inside\".\n",
    "        inside_label = \"I-\" + label[2:]\n",
    "        return [label] + ([inside_label] * (repetitions - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840ac4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p><big>üñõ The maximum number of tokens in one sentence that <small><code>bert-base-uncased</code></small> supports is 512. After tokenising, some sentences are going to have more than 512 tokens. I have verified (not shown here) that it only affects a slight minority of them, as almost all figure captions will fit under this threshold.\n",
    "<p>For simplicity, here I just pad+truncate all sentences to 512 sub-word tokens. However, there are of course more clever ways to do this: we could choose a larger model which supports longer context, or we could have broken down long sentences into several smaller ones (with overlap) and then stiched it back together.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2dd545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_and_convert_labels(row):\n",
    "    \"\"\"Tokenise words and convert all word IOB labels into token IOB labels.\"\"\"\n",
    "    \n",
    "    # Make sure that rows and labels match.\n",
    "    assert len(row.words) == len(row.labels), \"The lengths of words and labels vectors do not match\"\n",
    "    \n",
    "    # Tokenise.\n",
    "    tokens = tokeniser(\n",
    "        row.words,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    # Calculate how many tokens does each word have.\n",
    "    word_token_count = collections.Counter(tokens.word_ids())\n",
    "\n",
    "    # Compute token IOB labels.\n",
    "    token_iob_labels = []\n",
    "    for i, label in enumerate(row.labels_str):\n",
    "        token_iob_labels.extend(\n",
    "            repeat_word_label(label, word_token_count[i])\n",
    "        )\n",
    " \n",
    "    # Pad labels to match special tokens: [CLS], [SEP], and [PAD] (multiple).\n",
    "    token_iob_labels = [None] + token_iob_labels + [None]\n",
    "    token_iob_labels += [None] * (512 - len(token_iob_labels))\n",
    "    token_iob_labels = token_iob_labels[:512]\n",
    "    \n",
    "    return tokens.input_ids, token_iob_labels, tokens.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad9ff723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in ALL_SUBSETS:\n",
    "    df[subset][[\"tokens\", \"token_labels\", \"attention_mask\"]] = df[subset].parallel_apply(\n",
    "        tokenise_and_convert_labels, axis=1, result_type=\"expand\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c453748b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_str</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_labels</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B, -, D, Left, ventricular, LXRŒ±, expression,...</td>\n",
       "      <td>[0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...</td>\n",
       "      <td>[O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...</td>\n",
       "      <td>[101, 1038, 1011, 1040, 2187, 18834, 7277, 793...</td>\n",
       "      <td>[None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B, -, D, Left, ventricular, LXRŒ±, expression,...</td>\n",
       "      <td>[0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...</td>\n",
       "      <td>[O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...</td>\n",
       "      <td>[101, 1038, 1011, 1040, 2187, 18834, 7277, 793...</td>\n",
       "      <td>[None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[E, LV, weight, to, tibia, length, ratios, (, ...</td>\n",
       "      <td>[0, 9, 13, 0, 9, 0, 0, 0, 9, 0, 9, 0, 0, 0, 0,...</td>\n",
       "      <td>[O, B-TISSUE, B-EXP_ASSAY, O, B-TISSUE, O, O, ...</td>\n",
       "      <td>[101, 1041, 1048, 2615, 3635, 2000, 14841, 116...</td>\n",
       "      <td>[None, O, B-TISSUE, I-TISSUE, B-EXP_ASSAY, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[F, Assessment, of, mean, arterial, pressure, ...</td>\n",
       "      <td>[0, 0, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O, O, O...</td>\n",
       "      <td>[101, 1042, 7667, 1997, 2812, 25543, 3778, 100...</td>\n",
       "      <td>[None, O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[G, Representative, Masson, ', s, trichrome, a...</td>\n",
       "      <td>[0, 0, 13, 14, 14, 14, 0, 3, 4, 4, 0, 13, 0, 0...</td>\n",
       "      <td>[O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_ASSAY, ...</td>\n",
       "      <td>[101, 1043, 4387, 3742, 2239, 1005, 1055, 1301...</td>\n",
       "      <td>[None, O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_A...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60261</th>\n",
       "      <td>[(, D, ), E14, ., 5, cortices, stained, with, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 9, 13, 0, 0, 0, 3, 0, 0, 0,...</td>\n",
       "      <td>[O, O, O, O, O, O, B-TISSUE, B-EXP_ASSAY, O, O...</td>\n",
       "      <td>[101, 1006, 1040, 1007, 1041, 16932, 1012, 101...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, B-TISSUE, I-TISSUE...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60262</th>\n",
       "      <td>[(, E, ), Ratio, of, mitotic, (, PH3, +, ), to...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-GENE...</td>\n",
       "      <td>[101, 1006, 1041, 1007, 6463, 1997, 10210, 202...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60263</th>\n",
       "      <td>[(, F, Quantification, of, the, number, of, (,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 0, 0, 0...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-EXP_ASSAY, I-...</td>\n",
       "      <td>[101, 1006, 1042, 24110, 3775, 10803, 1997, 19...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, B-E...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60264</th>\n",
       "      <td>[G, ), Quantification, of, the, number, of, (,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-GENEPRO...</td>\n",
       "      <td>[101, 1043, 1007, 24110, 3775, 10803, 1997, 19...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60265</th>\n",
       "      <td>[(, H, ), Representative, whole, mount, images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-GENEPROD, O, ...</td>\n",
       "      <td>[101, 1006, 1044, 1007, 4387, 2878, 4057, 4871...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, B-GENE...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60266 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words  \\\n",
       "0      [B, -, D, Left, ventricular, LXRŒ±, expression,...   \n",
       "1      [B, -, D, Left, ventricular, LXRŒ±, expression,...   \n",
       "2      [E, LV, weight, to, tibia, length, ratios, (, ...   \n",
       "3      [F, Assessment, of, mean, arterial, pressure, ...   \n",
       "4      [G, Representative, Masson, ', s, trichrome, a...   \n",
       "...                                                  ...   \n",
       "60261  [(, D, ), E14, ., 5, cortices, stained, with, ...   \n",
       "60262  [(, E, ), Ratio, of, mitotic, (, PH3, +, ), to...   \n",
       "60263  [(, F, Quantification, of, the, number, of, (,...   \n",
       "60264  [G, ), Quantification, of, the, number, of, (,...   \n",
       "60265  [(, H, ), Representative, whole, mount, images...   \n",
       "\n",
       "                                                  labels  \\\n",
       "0      [0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...   \n",
       "1      [0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...   \n",
       "2      [0, 9, 13, 0, 9, 0, 0, 0, 9, 0, 9, 0, 0, 0, 0,...   \n",
       "3      [0, 0, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4      [0, 0, 13, 14, 14, 14, 0, 3, 4, 4, 0, 13, 0, 0...   \n",
       "...                                                  ...   \n",
       "60261  [0, 0, 0, 0, 0, 0, 9, 13, 0, 0, 0, 3, 0, 0, 0,...   \n",
       "60262  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...   \n",
       "60263  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 0, 0, 0...   \n",
       "60264  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...   \n",
       "60265  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, ...   \n",
       "\n",
       "                                              labels_str  \\\n",
       "0      [O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...   \n",
       "1      [O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...   \n",
       "2      [O, B-TISSUE, B-EXP_ASSAY, O, B-TISSUE, O, O, ...   \n",
       "3      [O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O, O, O...   \n",
       "4      [O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_ASSAY, ...   \n",
       "...                                                  ...   \n",
       "60261  [O, O, O, O, O, O, B-TISSUE, B-EXP_ASSAY, O, O...   \n",
       "60262  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-GENE...   \n",
       "60263  [O, O, O, O, O, O, O, O, O, O, B-EXP_ASSAY, I-...   \n",
       "60264  [O, O, O, O, O, O, O, O, O, O, O, O, B-GENEPRO...   \n",
       "60265  [O, O, O, O, O, O, O, O, O, O, B-GENEPROD, O, ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [101, 1038, 1011, 1040, 2187, 18834, 7277, 793...   \n",
       "1      [101, 1038, 1011, 1040, 2187, 18834, 7277, 793...   \n",
       "2      [101, 1041, 1048, 2615, 3635, 2000, 14841, 116...   \n",
       "3      [101, 1042, 7667, 1997, 2812, 25543, 3778, 100...   \n",
       "4      [101, 1043, 4387, 3742, 2239, 1005, 1055, 1301...   \n",
       "...                                                  ...   \n",
       "60261  [101, 1006, 1040, 1007, 1041, 16932, 1012, 101...   \n",
       "60262  [101, 1006, 1041, 1007, 6463, 1997, 10210, 202...   \n",
       "60263  [101, 1006, 1042, 24110, 3775, 10803, 1997, 19...   \n",
       "60264  [101, 1043, 1007, 24110, 3775, 10803, 1997, 19...   \n",
       "60265  [101, 1006, 1044, 1007, 4387, 2878, 4057, 4871...   \n",
       "\n",
       "                                            token_labels  \\\n",
       "0      [None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...   \n",
       "1      [None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...   \n",
       "2      [None, O, B-TISSUE, I-TISSUE, B-EXP_ASSAY, O, ...   \n",
       "3      [None, O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O...   \n",
       "4      [None, O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_A...   \n",
       "...                                                  ...   \n",
       "60261  [None, O, O, O, O, O, O, O, B-TISSUE, I-TISSUE...   \n",
       "60262  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "60263  [None, O, O, O, O, O, O, O, O, O, O, O, O, B-E...   \n",
       "60264  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "60265  [None, O, O, O, O, O, O, O, O, O, O, O, B-GENE...   \n",
       "\n",
       "                                          attention_mask  \n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "...                                                  ...  \n",
       "60261  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "60262  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "60263  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "60264  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "60265  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "\n",
       "[60266 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ed5db",
   "metadata": {},
   "source": [
    "## 5.4. Encode the token IOB labels\n",
    "<big>Because the token labels are now still strings (IOB), we need to encode them as integers for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca057e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-SMALL_MOLECULE': 2,\n",
       " 'I-SMALL_MOLECULE': 3,\n",
       " 'B-GENEPROD': 4,\n",
       " 'I-GENEPROD': 5,\n",
       " 'B-SUBCELLULAR': 6,\n",
       " 'I-SUBCELLULAR': 7,\n",
       " 'B-CELL_TYPE': 8,\n",
       " 'I-CELL_TYPE': 9,\n",
       " 'B-TISSUE': 10,\n",
       " 'I-TISSUE': 11,\n",
       " 'B-ORGANISM': 12,\n",
       " 'I-ORGANISM': 13,\n",
       " 'B-EXP_ASSAY': 14,\n",
       " 'I-EXP_ASSAY': 15,\n",
       " 'B-DISEASE': 16,\n",
       " 'I-DISEASE': 17,\n",
       " 'B-CELL_LINE': 18,\n",
       " 'I-CELL_LINE': 19,\n",
       " None: 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_label_encoding = {label: i for i, label in enumerate(LABELS, 1)}\n",
    "token_label_encoding[None] = 0  # For special tokens like [CLS], [SEP], [PAD].\n",
    "token_label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1551ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need this later when we want to go back from indexes to the source labels.\n",
    "token_label_decoding = {v: k for k, v in token_label_encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "888f87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_token_labels(token_labels):\n",
    "    return [token_label_encoding[x] for x in token_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacbb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in ALL_SUBSETS:\n",
    "    df[subset][\"encoded_token_labels\"] = df[subset][\"token_labels\"].parallel_apply(encode_token_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e735cd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>labels_str</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_labels</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>encoded_token_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B, -, D, Left, ventricular, LXRŒ±, expression,...</td>\n",
       "      <td>[0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...</td>\n",
       "      <td>[O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...</td>\n",
       "      <td>[101, 1038, 1011, 1040, 2187, 18834, 7277, 793...</td>\n",
       "      <td>[None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 10, 11, 11, 11, 4, 5, 5, 5, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B, -, D, Left, ventricular, LXRŒ±, expression,...</td>\n",
       "      <td>[0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...</td>\n",
       "      <td>[O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...</td>\n",
       "      <td>[101, 1038, 1011, 1040, 2187, 18834, 7277, 793...</td>\n",
       "      <td>[None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 10, 11, 11, 11, 4, 5, 5, 5, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[E, LV, weight, to, tibia, length, ratios, (, ...</td>\n",
       "      <td>[0, 9, 13, 0, 9, 0, 0, 0, 9, 0, 9, 0, 0, 0, 0,...</td>\n",
       "      <td>[O, B-TISSUE, B-EXP_ASSAY, O, B-TISSUE, O, O, ...</td>\n",
       "      <td>[101, 1041, 1048, 2615, 3635, 2000, 14841, 116...</td>\n",
       "      <td>[None, O, B-TISSUE, I-TISSUE, B-EXP_ASSAY, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 10, 11, 14, 1, 10, 11, 1, 1, 1, 10, 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[F, Assessment, of, mean, arterial, pressure, ...</td>\n",
       "      <td>[0, 0, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O, O, O...</td>\n",
       "      <td>[101, 1042, 7667, 1997, 2812, 25543, 3778, 100...</td>\n",
       "      <td>[None, O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 14, 15, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[G, Representative, Masson, ', s, trichrome, a...</td>\n",
       "      <td>[0, 0, 13, 14, 14, 14, 0, 3, 4, 4, 0, 13, 0, 0...</td>\n",
       "      <td>[O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_ASSAY, ...</td>\n",
       "      <td>[101, 1043, 4387, 3742, 2239, 1005, 1055, 1301...</td>\n",
       "      <td>[None, O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_A...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 14, 15, 15, 15, 15, 15, 1, 4, 5, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60261</th>\n",
       "      <td>[(, D, ), E14, ., 5, cortices, stained, with, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 9, 13, 0, 0, 0, 3, 0, 0, 0,...</td>\n",
       "      <td>[O, O, O, O, O, O, B-TISSUE, B-EXP_ASSAY, O, O...</td>\n",
       "      <td>[101, 1006, 1040, 1007, 1041, 16932, 1012, 101...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, B-TISSUE, I-TISSUE...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 10, 11, 11, 14, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60262</th>\n",
       "      <td>[(, E, ), Ratio, of, mitotic, (, PH3, +, ), to...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B-GENE...</td>\n",
       "      <td>[101, 1006, 1041, 1007, 6463, 1997, 10210, 202...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60263</th>\n",
       "      <td>[(, F, Quantification, of, the, number, of, (,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 0, 0, 0...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-EXP_ASSAY, I-...</td>\n",
       "      <td>[101, 1006, 1042, 24110, 3775, 10803, 1997, 19...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, B-E...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14, 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60264</th>\n",
       "      <td>[G, ), Quantification, of, the, number, of, (,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-GENEPRO...</td>\n",
       "      <td>[101, 1043, 1007, 24110, 3775, 10803, 1997, 19...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60265</th>\n",
       "      <td>[(, H, ), Representative, whole, mount, images...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-GENEPROD, O, ...</td>\n",
       "      <td>[101, 1006, 1044, 1007, 4387, 2878, 4057, 4871...</td>\n",
       "      <td>[None, O, O, O, O, O, O, O, O, O, O, O, B-GENE...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 5, 5, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60266 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words  \\\n",
       "0      [B, -, D, Left, ventricular, LXRŒ±, expression,...   \n",
       "1      [B, -, D, Left, ventricular, LXRŒ±, expression,...   \n",
       "2      [E, LV, weight, to, tibia, length, ratios, (, ...   \n",
       "3      [F, Assessment, of, mean, arterial, pressure, ...   \n",
       "4      [G, Representative, Masson, ', s, trichrome, a...   \n",
       "...                                                  ...   \n",
       "60261  [(, D, ), E14, ., 5, cortices, stained, with, ...   \n",
       "60262  [(, E, ), Ratio, of, mitotic, (, PH3, +, ), to...   \n",
       "60263  [(, F, Quantification, of, the, number, of, (,...   \n",
       "60264  [G, ), Quantification, of, the, number, of, (,...   \n",
       "60265  [(, H, ), Representative, whole, mount, images...   \n",
       "\n",
       "                                                  labels  \\\n",
       "0      [0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...   \n",
       "1      [0, 0, 0, 9, 10, 3, 0, 0, 0, 0, 3, 0, 0, 11, 0...   \n",
       "2      [0, 9, 13, 0, 9, 0, 0, 0, 9, 0, 9, 0, 0, 0, 0,...   \n",
       "3      [0, 0, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4      [0, 0, 13, 14, 14, 14, 0, 3, 4, 4, 0, 13, 0, 0...   \n",
       "...                                                  ...   \n",
       "60261  [0, 0, 0, 0, 0, 0, 9, 13, 0, 0, 0, 3, 0, 0, 0,...   \n",
       "60262  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...   \n",
       "60263  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 0, 0, 0...   \n",
       "60264  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, ...   \n",
       "60265  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, ...   \n",
       "\n",
       "                                              labels_str  \\\n",
       "0      [O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...   \n",
       "1      [O, O, O, B-TISSUE, I-TISSUE, B-GENEPROD, O, O...   \n",
       "2      [O, B-TISSUE, B-EXP_ASSAY, O, B-TISSUE, O, O, ...   \n",
       "3      [O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O, O, O...   \n",
       "4      [O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_ASSAY, ...   \n",
       "...                                                  ...   \n",
       "60261  [O, O, O, O, O, O, B-TISSUE, B-EXP_ASSAY, O, O...   \n",
       "60262  [O, O, O, O, O, O, O, O, O, O, O, O, O, B-GENE...   \n",
       "60263  [O, O, O, O, O, O, O, O, O, O, B-EXP_ASSAY, I-...   \n",
       "60264  [O, O, O, O, O, O, O, O, O, O, O, O, B-GENEPRO...   \n",
       "60265  [O, O, O, O, O, O, O, O, O, O, B-GENEPROD, O, ...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [101, 1038, 1011, 1040, 2187, 18834, 7277, 793...   \n",
       "1      [101, 1038, 1011, 1040, 2187, 18834, 7277, 793...   \n",
       "2      [101, 1041, 1048, 2615, 3635, 2000, 14841, 116...   \n",
       "3      [101, 1042, 7667, 1997, 2812, 25543, 3778, 100...   \n",
       "4      [101, 1043, 4387, 3742, 2239, 1005, 1055, 1301...   \n",
       "...                                                  ...   \n",
       "60261  [101, 1006, 1040, 1007, 1041, 16932, 1012, 101...   \n",
       "60262  [101, 1006, 1041, 1007, 6463, 1997, 10210, 202...   \n",
       "60263  [101, 1006, 1042, 24110, 3775, 10803, 1997, 19...   \n",
       "60264  [101, 1043, 1007, 24110, 3775, 10803, 1997, 19...   \n",
       "60265  [101, 1006, 1044, 1007, 4387, 2878, 4057, 4871...   \n",
       "\n",
       "                                            token_labels  \\\n",
       "0      [None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...   \n",
       "1      [None, O, O, O, B-TISSUE, I-TISSUE, I-TISSUE, ...   \n",
       "2      [None, O, B-TISSUE, I-TISSUE, B-EXP_ASSAY, O, ...   \n",
       "3      [None, O, O, O, O, B-EXP_ASSAY, I-EXP_ASSAY, O...   \n",
       "4      [None, O, O, B-EXP_ASSAY, I-EXP_ASSAY, I-EXP_A...   \n",
       "...                                                  ...   \n",
       "60261  [None, O, O, O, O, O, O, O, B-TISSUE, I-TISSUE...   \n",
       "60262  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "60263  [None, O, O, O, O, O, O, O, O, O, O, O, O, B-E...   \n",
       "60264  [None, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "60265  [None, O, O, O, O, O, O, O, O, O, O, O, B-GENE...   \n",
       "\n",
       "                                          attention_mask  \\\n",
       "0      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                  ...   \n",
       "60261  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "60262  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "60263  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "60264  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "60265  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                    encoded_token_labels  \n",
       "0      [0, 1, 1, 1, 10, 11, 11, 11, 4, 5, 5, 5, 1, 1,...  \n",
       "1      [0, 1, 1, 1, 10, 11, 11, 11, 4, 5, 5, 5, 1, 1,...  \n",
       "2      [0, 1, 10, 11, 14, 1, 10, 11, 1, 1, 1, 10, 11,...  \n",
       "3      [0, 1, 1, 1, 1, 14, 15, 1, 1, 1, 1, 1, 1, 1, 1...  \n",
       "4      [0, 1, 1, 14, 15, 15, 15, 15, 15, 1, 4, 5, 5, ...  \n",
       "...                                                  ...  \n",
       "60261  [0, 1, 1, 1, 1, 1, 1, 1, 10, 11, 11, 14, 1, 1,...  \n",
       "60262  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "60263  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14, 15...  \n",
       "60264  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "60265  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 5, 5, ...  \n",
       "\n",
       "[60266 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61033a",
   "metadata": {},
   "source": [
    "# 6. Set up framework for hyperparameter tuning and training\n",
    "<big>We are going to be repeatedly training the model while varying hyperparameters and benchmarking it on the validation set to find the best combination. As such, it makes sense to set up a framework so that we can easily run training/validation and get the results.\n",
    "\n",
    "<big>As a criteria for model \"quality\", I will be using an average macro F1 score. This should overall assess model quality well, taking into account imbalances between classes.\n",
    "\n",
    "<big>One we have the best final model, we will look into a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b46ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceData:\n",
    "    \n",
    "    def __init__(self, df, num_labels, f1_aggregation, save_prefix, model=None,\n",
    "                 truncate_subsets=None, is_distil_bert=False):\n",
    "        \"\"\"Initialise the class.\"\"\"\n",
    "        self.df = df\n",
    "        self.num_labels = num_labels\n",
    "        self.f1_aggregation = f1_aggregation\n",
    "        self.save_prefix = save_prefix\n",
    "        \n",
    "        # If a model is provided, then loaded; otherwise initialise a base model.\n",
    "        if model:\n",
    "            if is_distil_bert:\n",
    "                self.model = DistilBertForTokenClassification.from_pretrained(model)\n",
    "            else:\n",
    "                self.model = BertForTokenClassification.from_pretrained(model)\n",
    "        else:\n",
    "            self.base_model()\n",
    "        \n",
    "        # To speed up calculations, we might want to truncate some of the subsets.\n",
    "        if truncate_subsets:\n",
    "            for subset, num_of_records in truncate_subsets.items():\n",
    "                self.df[subset] = self.df[subset].head(num_of_records)        \n",
    "    \n",
    "    def base_model(self):\n",
    "        self.model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=self.num_labels)\n",
    "    \n",
    "    def get_tensors(self, subset):\n",
    "        \"\"\"Prepare tensors for the specified subset: tokens, attention mask, labels.\"\"\"\n",
    "        return [\n",
    "            torch.tensor(\n",
    "                self.df[subset][column].tolist()\n",
    "            )\n",
    "            for column in (\n",
    "                \"tokens\", \"attention_mask\", \"encoded_token_labels\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def collapse_labels(self, label_indices):\n",
    "        \"\"\"Convert label indices to the source labels and collapse I-SOMECLASS/B-SOMECLASS into SOMECLASS.\"\"\"\n",
    "        labels = [token_label_decoding[i] for i in label_indices]\n",
    "        collapsed_labels = [\n",
    "            (\n",
    "                str(label[2:])\n",
    "                if str(label).startswith(\"I-\") or str(label).startswith(\"B-\")\n",
    "                else str(label)\n",
    "            )\n",
    "            for label in labels\n",
    "        ]\n",
    "        return collapsed_labels\n",
    "\n",
    "    def evaluate(self, device, subset, collapse_labels=False):\n",
    "        \"\"\"Check model's aggregated F1 performance.\"\"\"\n",
    "        \n",
    "        # Set up hardware.\n",
    "        device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Put model into evaluation mode.\n",
    "        self.model.eval()\n",
    "                \n",
    "        # Prepare data.\n",
    "        val_dataset = TensorDataset(*(self.get_tensors(subset)))\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        # Lists to store predictions and true labels.\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "\n",
    "        # Iterate over the validation set.\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, f\"[evaluate] [{subset}]\"):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
    "\n",
    "                # Forward pass.\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Get predictions, attention mask, true labels.\n",
    "                predictions = torch.argmax(logits, dim=2).cpu().numpy().flatten()\n",
    "                attention_mask = batch[1].cpu().numpy().flatten()\n",
    "                true_labels = batch[2].cpu().numpy().flatten()\n",
    "\n",
    "                # Exclude padding tokens from calculations.\n",
    "                all_predictions.extend(predictions[attention_mask == 1])\n",
    "                all_true_labels.extend(true_labels[attention_mask == 1])\n",
    "\n",
    "        if collapse_labels:\n",
    "            all_predictions = self.collapse_labels(all_predictions)\n",
    "            all_true_labels = self.collapse_labels(all_true_labels)\n",
    "                \n",
    "        # Calculate and return the classification report and the F1 score.\n",
    "        f1 = sklearn.metrics.f1_score(\n",
    "            all_true_labels,\n",
    "            all_predictions,\n",
    "            average=self.f1_aggregation,\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            # Suppress warnings when some labels did not occur in the predicted values.\n",
    "            warnings.simplefilter(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "            classification_report = sklearn.metrics.classification_report(\n",
    "                all_true_labels,\n",
    "                all_predictions,\n",
    "            )\n",
    "        return f1, classification_report\n",
    "\n",
    "    def train_validate_save(self, device, num_epochs, learning_rate, batch_size):\n",
    "        \"\"\"Runs several epochs of training the model with given learning rate and batch size.\n",
    "        After each training cycle, also computes F1 score.\n",
    "        Returns a list of F1 scores for each training epoch.\"\"\"\n",
    "        \n",
    "        # Prepare data.\n",
    "        train_dataset = TensorDataset(*(self.get_tensors(\"train\")))\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialise the optimiser.\n",
    "        optimiser = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Set up hardware.\n",
    "        device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Train/validate/save cycle.\n",
    "        f1_values = []\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            \n",
    "            # Train.\n",
    "            self.model.train()\n",
    "            tqdm_desc = (\n",
    "                f\"[train] \"\n",
    "                f\"lr {learning_rate}, \"\n",
    "                f\"bs {batch_size}, \"\n",
    "                f\"epoch {epoch}/{num_epochs}\"\n",
    "            )\n",
    "            for batch in tqdm(train_dataloader, desc=tqdm_desc):\n",
    "                optimiser.zero_grad()\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                outputs = self.model(input_ids=batch[0], attention_mask=batch[1], labels=batch[2])\n",
    "                loss = outputs[0]\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                \n",
    "            # Validate.\n",
    "            f1, _ = self.evaluate(device=device, subset=\"validation\", collapse_labels=True)\n",
    "            f1_values.append(f1)\n",
    "            \n",
    "            # Save.\n",
    "            self.model.save_pretrained(f\"{self.save_prefix}_{learning_rate}_{batch_size}_{epoch}\")\n",
    "        \n",
    "        # Clean up memory and return.\n",
    "        del self.model\n",
    "        self.empty_model()\n",
    "        return f1_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bc400",
   "metadata": {},
   "source": [
    "# 7. Train model and tune hyperparameters\n",
    "\n",
    "<big>First, we set some common parameters to be used throughout the process.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p><big> üñõ Although we have a substantial training set size of more than 60,000 records, it's not feasible (for a technical exercise) to train on the entire set, especially considering that we want to tune hyperparameters. This is because, even on a GPU, it takes about 1.5 hours for one training epoch on the full 60,000 records.\n",
    "<p>Because of that, I truncate the subsets as described below. This will, of course, degrate model performance, but will not prevent us from demonstrating all of the basic principles. If we wanted to run the same code on a bigger dataset, it's just the matter of adjusting or removing constraints below, and then waiting for longer.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p><big>üñõ While I use simple truncation for simplicity, we could also have used more sophisticated ways to cut down a training set. For example, we could try to use stratified sampling to make sure the classes are more evenly present in the skimmed down training set, but again I feel this is outside of the scope of this technical exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d16913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncating subsets for increased performance.\n",
    "TRUNCATE_TRAIN = 6144\n",
    "TRUNCATE_VALIDATION = 1536\n",
    "# Test set is not truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2166a8d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p><big>üñõ For a production task, I would have varied the hyperparameters more (not just a few values) and increased the number of epochs.\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "<p><big>üñõ Rather than varying the parameters manually, it is possible to use frameworks like Optuna, but again I feel this is an overkill for the scope of this exercise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1ad120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATES = [\n",
    "    3e-5,\n",
    "    5e-5,\n",
    "]\n",
    "BATCH_SIZES = [\n",
    "    8,\n",
    "    16,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad86ac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p><big>üñõ The reason I chose to use macro aggregation for the F1 metrics is because it seems to be the least sensitive to the over-abundance of the <small><code>O</code></small> class, and more sensitive to how well the actual entity classes are predicted. Overall it seems to be the most ‚Äúpessimistic‚Äù metric, which I thought would be good to better reflect the training progress.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd18e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation for F1.\n",
    "F1_AGGREGATION = \"macro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4767e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix for saving the models.\n",
    "SAVE_PREFIX = \"model_v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ad5f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] lr 3e-05, bs 8, epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [09:38<00:00,  1.33it/s]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.05s/it]\n",
      "[train] lr 3e-05, bs 8, epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [09:38<00:00,  1.33it/s]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.05s/it]\n",
      "[train] lr 3e-05, bs 8, epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [09:38<00:00,  1.33it/s]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.05s/it]\n",
      "[train] lr 3e-05, bs 16, epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [09:28<00:00,  1.48s/it]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:51<00:00,  1.07s/it]\n",
      "[train] lr 3e-05, bs 16, epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [09:28<00:00,  1.48s/it]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:51<00:00,  1.07s/it]\n",
      "[train] lr 3e-05, bs 16, epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [09:27<00:00,  1.48s/it]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:51<00:00,  1.07s/it]\n",
      "[train] lr 5e-05, bs 8, epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [09:38<00:00,  1.33it/s]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.05s/it]\n",
      "[train] lr 5e-05, bs 8, epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [09:37<00:00,  1.33it/s]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.05s/it]\n",
      "[train] lr 5e-05, bs 8, epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 768/768 [09:37<00:00,  1.33it/s]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.05s/it]\n",
      "[train] lr 5e-05, bs 16, epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [09:27<00:00,  1.48s/it]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:51<00:00,  1.06s/it]\n",
      "[train] lr 5e-05, bs 16, epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [09:27<00:00,  1.48s/it]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:51<00:00,  1.06s/it]\n",
      "[train] lr 5e-05, bs 16, epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [09:27<00:00,  1.48s/it]\n",
      "[evaluate] [validation]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:50<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "SD = SourceData(\n",
    "    df=df,\n",
    "    num_labels=len(token_label_encoding),\n",
    "    f1_aggregation=F1_AGGREGATION,\n",
    "    save_prefix=SAVE_PREFIX,\n",
    "    truncate_subsets={\"train\": TRUNCATE_TRAIN, \"validation\": TRUNCATE_VALIDATION}\n",
    ")\n",
    "f1_data = []\n",
    "\n",
    "for learning_rate in LEARNING_RATES:\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        f1_scores = SD.train_validate_save(\n",
    "            device=\"cuda\",\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        for epoch, f1 in enumerate(f1_scores, 1):\n",
    "            f1_data.append([learning_rate, batch_size, epoch, f1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2ac9c",
   "metadata": {},
   "source": [
    "<big>We can now visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0c9c6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epoch</th>\n",
       "      <th>agg_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.744670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.781494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.779147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.693277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.770304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.767005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.779534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.776964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.718923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.775561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00005</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0.777572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  batch_size  epoch    agg_f1\n",
       "0         0.00003           8      1  0.744670\n",
       "1         0.00003           8      2  0.781494\n",
       "2         0.00003           8      3  0.779147\n",
       "3         0.00003          16      1  0.693277\n",
       "4         0.00003          16      2  0.770304\n",
       "5         0.00003          16      3  0.767005\n",
       "6         0.00005           8      1  0.753977\n",
       "7         0.00005           8      2  0.779534\n",
       "8         0.00005           8      3  0.776964\n",
       "9         0.00005          16      1  0.718923\n",
       "10        0.00005          16      2  0.775561\n",
       "11        0.00005          16      3  0.777572"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_df = pd.DataFrame(f1_data, columns=[\"learning_rate\", \"batch_size\", \"epoch\", \"agg_f1\"])\n",
    "f1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85aa85f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grouping by unique combinations of learning_rate and batch_size.\n",
    "grouped_df = f1_df.groupby([\"learning_rate\", \"batch_size\"])\n",
    "\n",
    "# Define line styles for better visibility.\n",
    "line_styles = ['-', '--', '-.', ':']\n",
    "\n",
    "# Plotting for each group with different line styles.\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (name, group) in enumerate(grouped_df):\n",
    "    learning_rate, batch_size = name\n",
    "    line_style = line_styles[i % len(line_styles)]\n",
    "    ax.plot(group[\"epoch\"], group[\"agg_f1\"], label=f\"LR={learning_rate}, Batch={batch_size}\", linestyle=line_style)\n",
    "\n",
    "# Adding labels and legend.\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(f\"Aggregated F1 ({F1_AGGREGATION})\")\n",
    "ax.set_title(f\"Aggregated F1 ({F1_AGGREGATION}) vs Epoch for Different Learning Rates and Batch Sizes\")\n",
    "ax.legend()\n",
    "\n",
    "# Setting the X-axis format as integer.\n",
    "ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
    "\n",
    "# Setting ticks every 1 (no fractional values).\n",
    "ax.set_xticks(np.arange(min(f1_df[\"epoch\"]), max(f1_df[\"epoch\"])+1, 1.0))\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438d64d",
   "metadata": {},
   "source": [
    "# 8. Evalutate the best model\n",
    "\n",
    "<big>Based on the results of the above comparison, we can now detect and evaluate the best model, and see how it performs on various classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e7028d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is model_v4_3e-05_8_2\n"
     ]
    }
   ],
   "source": [
    "# Detect the best model.\n",
    "max_index = f1_df['agg_f1'].idxmax()\n",
    "row = f1_df.loc[max_index]\n",
    "best_model = f\"{SAVE_PREFIX}_{row.learning_rate}_{int(row.batch_size)}_{int(row.epoch)}\"\n",
    "print(f\"The best model is {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10c5bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[evaluate] [test]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [03:37<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "SD_best = SourceData(\n",
    "    df=df,\n",
    "    num_labels=len(token_label_encoding),\n",
    "    f1_aggregation=F1_AGGREGATION,\n",
    "    save_prefix=SAVE_PREFIX,\n",
    "    model=f\"{os.getcwd()}/{best_model}\"\n",
    ")\n",
    "f1_score, classification_report = SD_best.evaluate(device=\"cuda\", subset=\"test\", collapse_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b4e77a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CELL_LINE       0.86      0.84      0.85      7640\n",
      "     CELL_TYPE       0.74      0.72      0.73      7903\n",
      "       DISEASE       0.56      0.55      0.56      1791\n",
      "     EXP_ASSAY       0.80      0.76      0.78     34215\n",
      "      GENEPROD       0.87      0.84      0.85     70668\n",
      "          None       1.00      1.00      1.00     13386\n",
      "             O       0.96      0.96      0.96    487235\n",
      "      ORGANISM       0.87      0.72      0.78      7948\n",
      "SMALL_MOLECULE       0.62      0.81      0.70     19728\n",
      "   SUBCELLULAR       0.68      0.73      0.70     10181\n",
      "        TISSUE       0.77      0.72      0.75      7813\n",
      "\n",
      "      accuracy                           0.92    668508\n",
      "     macro avg       0.79      0.79      0.79    668508\n",
      "  weighted avg       0.92      0.92      0.92    668508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5406698",
   "metadata": {},
   "source": [
    "<big>As we can see, performance of the model is quite good! It's even more impressive considering that:\n",
    "* <big>I used <small>`bert-base-uncased`</small> which isn't nearly state of the art even across open source models, and it only has 110M parameters;\n",
    "* <big>Training set was significantly constrained to increase performance;\n",
    "* <big>The number of hyperparameter combinations & epochs was not high.\n",
    "\n",
    "<big>Similarly to the results reported in the original paper, we can see that classes vary in their F1 scores quite significantly. This is due to the fact of them that some of them are more abundant, and/or more straightforward to grasp, than others. As in the paper, the worst performance that we observe is with the ‚Äúdisease‚Äù class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398078c",
   "metadata": {},
   "source": [
    "# 9. Optimising inference time via knowledge distillation\n",
    "\n",
    "<big>In production, we want the inference on the model to be running quickly, and depending on the task we might be willing to sacrifice some quality.\n",
    "\n",
    "<big>The idea behind model compression via knowledge distillation is that a smaller ‚Äústudent‚Äù model is trained with the larger model serving as a ‚Äúteacher‚Äù. A smaller model will not be as good as the large one, but we can hope to make it faster and achieve a good tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a26fcc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[distillation] 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1536/1536 [08:38<00:00,  2.96it/s]\n",
      "[distillation] 2/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1536/1536 [08:37<00:00,  2.97it/s]\n",
      "[distillation] 3/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1536/1536 [08:38<00:00,  2.96it/s]\n",
      "[distillation] 4/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1536/1536 [08:38<00:00,  2.96it/s]\n",
      "[distillation] 5/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1536/1536 [08:38<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare teacher and student models.\n",
    "teacher_model = SD_best.model\n",
    "student_model = DistilBertForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(token_label_encoding)\n",
    ")\n",
    "\n",
    "# Prepare input data.\n",
    "train_dataset = TensorDataset(*[\n",
    "    torch.tensor(\n",
    "        df[\"train\"][:TRUNCATE_TRAIN][column].tolist()\n",
    "    )\n",
    "    for column in (\n",
    "        \"tokens\", \"attention_mask\", \"encoded_token_labels\"\n",
    "    )\n",
    "])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Prepare devices.\n",
    "teacher_model.cuda()\n",
    "student_model.cuda()\n",
    "\n",
    "# Set the teacher model in evaluation mode.\n",
    "teacher_model.eval()\n",
    "\n",
    "# Define the loss function and optimizer.\n",
    "loss_function = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-5)\n",
    "distillation_epochs = 5\n",
    "\n",
    "# Start training loop.\n",
    "for epoch in range(1, distillation_epochs+1):\n",
    "    for batch in tqdm(train_dataloader, f\"[distillation] epoch {epoch}/{distillation_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move tensors to GPU.\n",
    "        input_ids = batch[0].cuda()\n",
    "        attention_mask = batch[1].cuda()\n",
    "        labels = batch[2].cuda()\n",
    "\n",
    "        # Forward pass through the student model.\n",
    "        student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Forward pass through the teacher model.\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Calculate the loss.\n",
    "        loss = loss_function(torch.log_softmax(student_outputs, dim=-1), torch.softmax(teacher_outputs, dim=-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a76c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the student model.\n",
    "student_model.eval()\n",
    "student_model.save_pretrained(f\"{os.getcwd()}/{SAVE_PREFIX}_student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99394afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[evaluate] [test]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 210/210 [01:49<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the student model on the test set.\n",
    "SD_student = SourceData(\n",
    "    df=df,\n",
    "    num_labels=len(token_label_encoding),\n",
    "    f1_aggregation=F1_AGGREGATION,\n",
    "    save_prefix=SAVE_PREFIX,\n",
    "    model=f\"{os.getcwd()}/{SAVE_PREFIX}_student\",\n",
    "    is_distil_bert=True,\n",
    ")\n",
    "f1_score, classification_report = SD_student.evaluate(device=\"cuda\", subset=\"test\", collapse_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dfac8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7772801545971528"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dbd2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CELL_LINE       0.92      0.74      0.82      7640\n",
      "     CELL_TYPE       0.74      0.65      0.69      7903\n",
      "       DISEASE       0.62      0.50      0.55      1791\n",
      "     EXP_ASSAY       0.80      0.76      0.78     34215\n",
      "      GENEPROD       0.85      0.85      0.85     70668\n",
      "          None       1.00      1.00      1.00     13386\n",
      "             O       0.96      0.96      0.96    487235\n",
      "      ORGANISM       0.87      0.71      0.78      7948\n",
      "SMALL_MOLECULE       0.63      0.74      0.68     19728\n",
      "   SUBCELLULAR       0.69      0.71      0.70     10181\n",
      "        TISSUE       0.75      0.72      0.73      7813\n",
      "\n",
      "      accuracy                           0.92    668508\n",
      "     macro avg       0.80      0.76      0.78    668508\n",
      "  weighted avg       0.92      0.92      0.92    668508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d41ae",
   "metadata": {},
   "source": [
    "<big>Results of knowledge distillation:\n",
    "| <big>Model | <big>Type | <big>Number of parameters | <big>Inference time | <big>Macro F1 | <big>Weighted F1 |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| <big>original | <big>BERT | <big>110M | <big>3m 37s | <big>0.79 | <big>0.92 |\n",
    "| <big>distilled | <big>DistilBERT | <big>66M | <big>1m 50s | <big> 0.78 | <big> 0.92 |\n",
    "| <big>(change) |  |  | <big>50% faster | <big>&lt;1% decrease | <big>no change |\n",
    "\n",
    "<big>The results are amazing: virtually no change in both macro and weighted average F1 scores (although there are some single percentage point degradations in individual classes), while inference is twice as fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11690052",
   "metadata": {},
   "source": [
    "# 10. Conclusion\n",
    "\n",
    "<big>I enjoyed doing this exercise with transformers. It was relatively easy to obtain good enough results even when aiming for quick execution time. The following improvements could be made if this were to be continued as a production grade task:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<big>üñõ <b>Main training stage</b>\n",
    "\n",
    "- Larger models as a base for main training stage\n",
    "- Models pretrained on biomedical knowledge such as PubMedBERT or BioLinkBERT\n",
    "- More epochs\n",
    "- Vary hyperparameters more\n",
    "- Use the full available training set\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<big>üñõ <b>Optimising for production</b>\n",
    "\n",
    "- Try an even smaller model than DistilBERT to see if we can make it work\n",
    "- Add some hyperparameter varying for this stage, too\n",
    "- More epochs for the distillation stage\n",
    "- Try to use quantisation to see if we can optimise further\n",
    "    </div>\n",
    "\n",
    "<big>I'm sure there are more things, but these are the ones I could think of.\n",
    "\n",
    "<big>That's all for now, and thank you for reading!<br>*‚Äî Kirill*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
